{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport zipfile\nimport sys\nimport time\nimport gc\nimport json\nfrom tempfile import TemporaryFile\n\nimport seaborn as sns\nimport spacy\nfrom sklearn import metrics as skm\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86ef0e07d7811dad271acf0b0fe7b90867eaada2"},"cell_type":"markdown","source":" # Data Path"},{"metadata":{"trusted":true,"_uuid":"a98de88f7a982ba72a389122c0a5006a5392da37"},"cell_type":"code","source":"DATA_ROOT = '../input/'\nGAP_DATA_FOLDER = os.path.join(DATA_ROOT, 'gap-coreference')\nSUB_DATA_FOLDER = os.path.join(DATA_ROOT, 'gendered-pronoun-resolution')\nFAST_TEXT_DATA_FOLDER = os.path.join(DATA_ROOT, 'fasttext-crawl-300d-2m')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbef9df2227e6d1a9119721a042b6f6c748513d1"},"cell_type":"code","source":"test_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-development.tsv')\ntrain_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-test.tsv')\ndev_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-validation.tsv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"252d2ea9dcf984fed9d02bce0e90ded5823b14f2"},"cell_type":"markdown","source":"# Clean Text"},{"metadata":{"_uuid":"e4b47891f9ad00a01ebf261cdb3088ae00e7930b"},"cell_type":"markdown","source":"## Clean up Entity Name"},{"metadata":{"trusted":true,"_uuid":"b032ad24b37464eeb341bc99b5dbbbba348827d8"},"cell_type":"code","source":"AT_NAME = \"AAAAAAXXXXXXXXX\"\nBT_NAME = \"BBBBBBXXXXXXXXX\"\nA_NAME = 'John'\nB_NAME = 'Bob'\n\ndef find_all_substring(a_str, sub):\n    start = 0\n    result = list()\n    while True:\n        start = a_str.find(sub, start)\n        if start == -1:\n            return result\n        result.append(start)\n        start += len(sub) # use start += 1 to find overlapping matches\n\ndef _update_offset(text, old_, new_, offset):\n    len_in = len(new_) - len(old_)\n    text_ = text[0:offset]\n    return offset + len_in * len(find_all_substring(text_, old_))\n    \ndef replace_entity_name(text, a_name, b_name, a_offset, b_offset, p_offset):\n    a_name = a_name.strip()\n    b_name = b_name.strip()\n    \n    if len(a_name) < len(b_name):\n        a_name, b_name = b_name, a_name\n        AT_NAME_, BT_NAME_ = BT_NAME, AT_NAME\n        A_NAME_, B_NAME_ = B_NAME, A_NAME\n    else:\n        AT_NAME_, BT_NAME_ = AT_NAME, BT_NAME\n        A_NAME_, B_NAME_ = A_NAME, B_NAME\n    \n    # replace the whole name\n    a_offset = _update_offset(text, a_name, AT_NAME_, a_offset)\n    b_offset = _update_offset(text, a_name, AT_NAME_, b_offset)\n    p_offset = _update_offset(text, a_name, AT_NAME_, p_offset)\n    text = text.replace(a_name, AT_NAME_)\n\n    a_offset = _update_offset(text, b_name, BT_NAME_, a_offset)\n    b_offset = _update_offset(text, b_name, BT_NAME_, b_offset)\n    p_offset = _update_offset(text, b_name, BT_NAME_, p_offset)\n    text = text.replace(b_name, BT_NAME_)\n    \n    # replace sub name\n    a_name_list = a_name.split(\" \")\n    b_name_list = b_name.split(\" \")\n    for a_subname in a_name_list:\n        a_offset = _update_offset(text, a_subname, AT_NAME_, a_offset)\n        b_offset = _update_offset(text, a_subname, AT_NAME_, b_offset)\n        p_offset = _update_offset(text, a_subname, AT_NAME_, p_offset)\n        text = text.replace(a_subname, AT_NAME_)\n    for b_subname in b_name_list:\n        a_offset = _update_offset(text, b_subname, BT_NAME_, a_offset)\n        b_offset = _update_offset(text, b_subname, BT_NAME_, b_offset)\n        p_offset = _update_offset(text, b_subname, BT_NAME_, p_offset)\n        text = text.replace(b_subname, BT_NAME_)\n    \n    # remove suffix\n    # replace the whole name\n    a_offset = _update_offset(text, AT_NAME_, A_NAME_, a_offset)\n    b_offset = _update_offset(text, AT_NAME_, A_NAME_, b_offset)\n    p_offset = _update_offset(text, AT_NAME_, A_NAME_, p_offset)\n    text = text.replace(AT_NAME_, A_NAME_)\n\n    a_offset = _update_offset(text, BT_NAME_, B_NAME_, a_offset)\n    b_offset = _update_offset(text, BT_NAME_, B_NAME_, b_offset)\n    p_offset = _update_offset(text, BT_NAME_, B_NAME_, p_offset)\n    text = text.replace(BT_NAME_, B_NAME_)\n    \n    if len(a_name) < len(b_name):\n        a_offset, b_offset = b_offset, a_offset\n    \n    return text, a_offset, b_offset, p_offset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97fadb216d66c26f548a251e8e2fe179198aa4b2"},"cell_type":"code","source":"def entity_replace_func(row):\n    text, a_offset, b_offset, p_offset = replace_entity_name(\n        row['Text'], row['A'], row['B'], row['A-offset'], row['B-offset'], row['Pronoun-offset']\n    )\n    \n    row_ = row.copy()\n    row_['Text'] = text\n    row_['A'] = A_NAME\n    row_['B'] = B_NAME\n    row_['A-offset'] = a_offset\n    row_['B-offset'] = b_offset\n    row_['Pronoun-offset'] = p_offset\n    \n    return row_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb848940ddd8189fae9187c781c40b68b028092a"},"cell_type":"markdown","source":"# Clip Text"},{"metadata":{"trusted":true,"_uuid":"54eb76b017c8623aaa809f64e0335ca0a49f4c8c"},"cell_type":"code","source":"train_df = pd.read_csv(train_df_path, sep='\\t')\nsns.distplot(train_df['Text'].map(lambda ele: len(ele.split(\" \"))), kde_kws={\"label\": \"text\"})\n\ndel train_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fcf1110b6b31cd65cc1dec7c3c730f83036c629"},"cell_type":"markdown","source":"Set max length to 150 covers most of cases. Clip text where the token length longer than 150."},{"metadata":{"trusted":true,"_uuid":"096666b7ecea234422d085a9ee95d40f85bb333b"},"cell_type":"code","source":"MAX_LEN = 150","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"461763372b5e807f349b8467bc205a7f5d98f6e9"},"cell_type":"code","source":"def bs(list_, target_):\n    lo, hi = 0, len(list_) -1\n    \n    while lo < hi:\n        mid = lo + int((hi - lo) / 2)\n        \n        if target_ < list_[mid]:\n            hi = mid\n        elif target_ > list_[mid]:\n            lo = mid + 1\n        else:\n            return mid + 1\n    return lo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28ac8b7602efbe080c87babe0e22d7cf5b573e32"},"cell_type":"code","source":"def clip_text(text, max_len, char_offset_p, char_offset_a, char_offset_b):\n    doc = nlp(text)\n    \n    if len(doc) <= max_len:\n        return text, 0\n    \n    token_lens = [token.idx for token in doc]\n    char_offset_min = min(char_offset_p, char_offset_a, char_offset_b)\n    char_offset_max = max(char_offset_p, char_offset_a, char_offset_b)\n    \n    # char offset to token offset\n    mention_offset_min = bs(token_lens, char_offset_min) - 1\n    mention_offset_max = bs(token_lens, char_offset_max) - 1\n    \n    if mention_offset_max - mention_offset_min + 1 > max_len:\n        raise ValueError\n    \n    # make sure the mention is in the sentence span\n    if mention_offset_max < max_len-1:\n        hi = doc[max_len].idx\n        return text[0:hi].strip(), 0\n    else:\n        len_span = mention_offset_max - mention_offset_min + 1\n        hi_idx = min((max_len - len_span) / 2 + mention_offset_max + 1, len(doc))\n        lo_idx = hi_idx - max_len\n        text_append = text + \" \"\n        return text_append[doc[lo_idx].idx: doc[hi_idx-1].idx + (doc[hi_idx-1].idx + len(doc[hi_idx-1]))].strip(), doc[lo_idx].idx\n    \ndef text_clip_func(row, max_len):\n    text, shift = clip_text(row['Text'], max_len, row['Pronoun-offset'], row['A-offset'], row['B-offset'])\n    return pd.Series([text, shift], index=['Text', 'Shift'])\n\ndef text_clip_update(df, max_len):\n    clip_info = df.apply(lambda row: text_clip_func(row, MAX_LEN), axis=1)\n    df['Text'] = clip_info['Text']\n    df['Pronoun-offset'] = df['Pronoun-offset'] - clip_info['Shift']\n    df['A-offset'] = df['A-offset'] - clip_info['Shift']\n    df['B-offset'] = df['B-offset'] - clip_info['Shift']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11abe2504b525c68ecc3faa820c7e2a96f1c4f2d"},"cell_type":"markdown","source":"# Encode By BERT"},{"metadata":{"_uuid":"26e811daa96d51bacaab58df081330063db24a2e"},"cell_type":"markdown","source":"Downloading the pre-trained BERT -Base, Uncased model. The kernel needs an Internet connection to do this, so make sure it's enabled."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#downloading weights and cofiguration file for the model\n!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\nwith zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n    zip_ref.extractall()\n!ls 'uncased_L-12_H-768_A-12'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"632a8531b58804cd28272c2ef8dd5b5102334bd7"},"cell_type":"markdown","source":"Next, in order to feed our data to the model, we'll use some scripts from the bert repo on GitHub."},{"metadata":{"trusted":true,"_uuid":"3e0ac6bb63d1487866640ebe8e73f78b3a96c25a"},"cell_type":"code","source":"!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/extract_features.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfccbec6c87185a0db428e3ce8ecb93aa9c4547e"},"cell_type":"code","source":"import modeling\nimport extract_features\nimport tokenization\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f525be88f8fa80c8a8b680c0946e111cd7f653fa"},"cell_type":"markdown","source":"Next, we feed BERT the data from these three files. For each line, we want to obtain contextual embeddings for the 3 target words (A, B, Pronoun). Here are some helper functions to keep track of the offsets of the target words."},{"metadata":{"trusted":true,"_uuid":"1a655a90d41802605da6f27c605313eac4af4cc2"},"cell_type":"code","source":"def compute_offset_no_spaces(text, offset):\n    count = 0\n    for pos in range(offset):\n        if text[pos] != \" \": count += 1\n    return count\n\n\ndef count_chars_no_special(text):\n    count = 0\n    special_char_list = [\"#\"]\n    for pos in range(len(text)):\n        if text[pos] not in special_char_list: count += 1\n    return count\n\n\ndef count_length_no_special(text):\n    count = 0\n    special_char_list = [\"#\", \" \"]\n    for pos in range(len(text)):\n        if text[pos] not in special_char_list: count += 1\n    return count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6552a7a9786187610be695fcf25766594ca41685"},"cell_type":"markdown","source":"The following method takes the data from a file, passes it through BERT to obtain contextual embeddings for the target words, then returns these embeddings in the emb DataFrame. Below, we will use it 3 times, once for each of the files gap-test, gap-development, gap-validation."},{"metadata":{"trusted":true,"_uuid":"9d8437cb4f7c1737e0c915d7d16cc3d004612416"},"cell_type":"code","source":"def encode_by_bert(data, embed_file_name):\n    '''\n    Runs a forward propagation of BERT on input text, extracting contextual word embeddings\n    Input: data, a pandas DataFrame containing the information in one of the GAP files\n\n    Output: emb, a pandas DataFrame containing contextual embeddings for the words A, B and Pronoun. Each embedding is a numpy array of shape (768)\n    columns: \"emb_A\": the embedding for word A\n             \"emb_B\": the embedding for word B\n             \"emb_P\": the embedding for the pronoun\n             \"label\": the answer to the coreference problem: \"A\", \"B\" or \"NEITHER\"\n    '''\n    # From the current file, take the text only, and write it in a file which will be passed to BERT\n    data[\"Text\"].to_csv(\"input.txt\", index=False, header=False)\n    # The script extract_features.py runs forward propagation through BERT, and writes the output in the file output.jsonl\n    # I'm lazy, so I'm only saving the output of the last layer. Feel free to change --layers = -1 to save the output of other layers.\n    os.system(\"python3 extract_features.py \\\n      --input_file=input.txt \\\n      --output_file=\" + embed_file_name + \" \\\n      --vocab_file=uncased_L-12_H-768_A-12/vocab.txt \\\n      --bert_config_file=uncased_L-12_H-768_A-12/bert_config.json \\\n      --init_checkpoint=uncased_L-12_H-768_A-12/bert_model.ckpt \\\n      --layers=-1 \\\n      --max_seq_length=\" + str(MAX_LEN + 10) + \" \\\n      --batch_size=8\")\n    os.system(\"rm input.txt\")\n\n    bert_output = pd.read_json(embed_file_name, lines=True)\n\n    index = data.index\n    columns = [\"emb_A\", \"emb_B\", \"emb_P\", \"label\"]\n    emb = pd.DataFrame(index=index, columns=columns)\n    emb.index.name = \"ID\"\n\n    rdata = np.zeros(shape=(len(data), 3))\n    for i in range(len(data)):  # For each line in the data file\n\n        # For each word, find the offset not counting spaces. This is necessary for comparison with the output of BERT\n        P_offset = compute_offset_no_spaces(data.loc[i, \"Text\"], data.loc[i, \"Pronoun-offset\"])\n        A_offset = compute_offset_no_spaces(data.loc[i, \"Text\"], data.loc[i, \"A-offset\"])\n        B_offset = compute_offset_no_spaces(data.loc[i, \"Text\"], data.loc[i, \"B-offset\"])\n\n        # Initialize counts\n        count_chars = 0\n\n        # find token index for P A and B\n        features = pd.DataFrame(\n            bert_output.loc[i, \"features\"])  # Get the BERT embeddings for the current line in the data file\n        times_to_try = 7\n        for j in range(2 if features.loc[1, \"token\"] == \"\\\"\" else 1, len(\n                features)):  # Iterate over the BERT  tokens for the current line; we skip over the first 2 tokens, which don't correspond to words\n            token = features.loc[j, \"token\"]\n\n            # See if the character count until the current token matches the offset of any of the 3 target words\n            if count_chars == P_offset:\n                found = False\n                for tt in range(times_to_try):\n                    if features.loc[max(0, j - tt), \"token\"].lower().strip() == data.iloc[i]['Pronoun'].lower().strip():\n                        rdata[i, 0] = max(0, j - tt)\n                        found = True\n                        break\n                if not found:\n                    for tt in range(times_to_try - 1):\n                        if features.loc[min(len(features) - 1, j + tt + 1), \"token\"].lower().strip() == data.iloc[i][\n                            'Pronoun'].lower().strip():\n                            rdata[i, 0] = min(len(features) - 1, j + tt + 1)\n                            found = True\n                            break\n                if not found:\n                    print(\"TOKEN NOT FOUND!\")\n                    print(features.loc[max(0, j - times_to_try):min(len(features) - 1, j + times_to_try), \"token\"])\n                    print(data.iloc[i]['Pronoun'])\n                    print(data.iloc[i]['Text'])\n                    print()\n\n            if count_chars == A_offset:\n                found = False\n                for tt in range(times_to_try):\n                    if features.loc[max(0, j - tt), \"token\"].lower().strip() == data.iloc[i]['A'].lower().strip():\n                        rdata[i, 1] = max(0, j - tt)\n                        found = True\n                        break\n                if not found:\n                    for tt in range(times_to_try - 1):\n                        if features.loc[min(len(features) - 1, j + tt + 1), \"token\"].lower().strip() == data.iloc[i][\n                            'A'].lower().strip():\n                            rdata[i, 1] = min(len(features) - 1, j + tt + 1)\n                            found = True\n                            break\n                if not found:\n                    print(\"TOKEN NOT FOUND!\")\n                    print(features.loc[max(0, j - times_to_try):min(len(features) - 1, j + times_to_try), \"token\"])\n                    print(data.iloc[i]['A'])\n                    print(data.iloc[i]['Text'])\n                    print()\n                    \n            if count_chars == B_offset:\n                found = False\n                for tt in range(times_to_try):\n                    if features.loc[max(0, j - tt), \"token\"].lower().strip() == data.iloc[i]['B'].lower().strip():\n                        rdata[i, 2] = max(0, j - tt)\n                        found = True\n                        break\n                if not found:\n                    for tt in range(times_to_try - 1):\n                        if features.loc[min(len(features) - 1, j + tt + 1), \"token\"].lower().strip() == data.iloc[i][\n                            'B'].lower().strip():\n                            rdata[i, 2] = min(len(features) - 1, j + tt + 1)\n                            found = True\n                            break\n                if not found:\n                    print(\"TOKEN NOT FOUND!\")\n                    print(features.loc[max(0, j - times_to_try):min(len(features) - 1, j + times_to_try), \"token\"])\n                    print(data.iloc[i]['B'])\n                    print(data.iloc[i]['Text'])\n                    print()\n            # Update the character count\n            count_chars += count_length_no_special(token)\n\n        data['Pronoun-index'] = rdata[:, 0]\n        data['A-index'] = rdata[:, 1]\n        data['B-index'] = rdata[:, 2]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab699e502663cf308f0fa9d4473412715370afb7"},"cell_type":"markdown","source":"## Split Embedding Files"},{"metadata":{"trusted":true,"_uuid":"f67ca1c6acae150770888d6c5aed66653549c161"},"cell_type":"code","source":"def batch_file_path(dst_folder, dataset_name, batch_index, file_format):\n    return dst_folder + \"/\" + dataset_name + '_' + str(batch_index) + file_format","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f38092181881f71c972d6787cbf87f82313bd2e3"},"cell_type":"code","source":"def split_embed_files(embed_file_name, batch_size, dst_folder, dataset_name):\n    bert_output = pd.read_json(embed_file_name, lines=True)\n    os.system(\"rm \" + embed_file_name)\n    \n    batch_matrix = list()\n    batch_index = 0\n    \n    # iterate through texts\n    for i in range(len(bert_output)):\n        features = pd.DataFrame(bert_output.loc[i, \"features\"])\n        batch_matrix.append(list())\n        # iterate through tokens\n        for j in range(0, len(features)):\n            batch_matrix[-1].append(features.loc[j,\"layers\"][0]['values'])\n        batch_matrix[-1] = np.asarray(batch_matrix[-1])\n        \n        if len(batch_matrix) == batch_size:\n            batch_matrix = np.asarray(batch_matrix)\n            np.save(batch_file_path(dst_folder, dataset_name, batch_index, \".npy\"), batch_matrix)\n            batch_matrix = list()\n            batch_index += 1\n        \n    if len(batch_matrix) > 0:\n        batch_matrix = np.asarray(batch_matrix)\n        np.save(batch_file_path(dst_folder, dataset_name, batch_index, \".npy\"), batch_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c665017e5b584ef0af4e73f8163193f0222994f7"},"cell_type":"code","source":"def map_to_batch_index(index, batch_size):\n    return index / batch_size","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98ca59d6fc859477aff649ea2ca8a35f65201e9d"},"cell_type":"markdown","source":"# Process Data"},{"metadata":{"trusted":true,"_uuid":"e93689d2877fdab7c3bcf994dbae35aeae0a62ff"},"cell_type":"code","source":"! mkdir embs\nembed_folder = \"embs\"\n\nnlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner', 'textcat'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fc498420e4780c67877071a69aec86cf7f993d6"},"cell_type":"markdown","source":"### Running Tests"},{"metadata":{"trusted":true,"_uuid":"2f1f3bc852afbcbf898ee6f95cba76dc49c19fc4"},"cell_type":"code","source":"# pd.options.display.max_colwidth = 1000\n\n# dev_df = pd.read_csv(dev_df_path, sep='\\t').drop(columns=['URL',])\n# # clean text\n# dev_df_ = dev_df.apply(entity_replace_func, axis=1)\n# # clip text\n# text_clip_update(dev_df_, MAX_LEN)\n\n# for i in range(dev_df_.shape[0]):\n#     row = dev_df_.iloc[i]\n#     text = row['Text']\n#     if text[row['A-offset']] != 'J':\n#         print('J')\n#         print(text)\n#         print(text[row['A-offset']:row['A-offset']+10] )\n#         print(dev_df.iloc[i][['Text', 'A', 'B']])\n#     if text[row['B-offset']] != 'B':\n#         print('B')\n#         print(text)\n#         print(text[row['B-offset']:row['B-offset']+10] )\n#         print(dev_df.iloc[i][['Text', 'A', 'B']])\n\n\n# # encode\n# dev_embed_file_name = \"dev_embed.json\"\n# encode_by_bert(dev_df_, dev_embed_file_name)\n# os.system(\"rm \" + dev_embed_file_name)\n# # split\n# # split_embed_files(dev_embed_file_name, 32, embed_folder, \"dev_embed\")\n# print(\"Finished at \", time.ctime())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8298947fa33285e722bdaae2df41bca5dd795732"},"cell_type":"code","source":"print(\"Started at \", time.ctime())\n\nnlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner', 'textcat'])\n\ntrain_embed_file_name = \"train_embed.json\"\ntest_embed_file_name = \"test_embed.json\"\n# dev_embed_file_name = \"dev_embed.json\"\n\nbatch_size =32\n\ntrain_df = pd.read_csv(train_df_path, sep='\\t').drop(columns=['URL',])\ndev_df = pd.read_csv(dev_df_path, sep='\\t').drop(columns=['URL',])\ntrain_df = pd.concat([train_df, dev_df], axis=0, ignore_index=True)\ntrain_df.reset_index(drop=True, inplace=True)\ndel dev_df\ngc.collect()\n# clean text\ntrain_df = train_df.apply(entity_replace_func, axis=1)\n# clip text\ntext_clip_update(train_df, MAX_LEN)\n# encode\nencode_by_bert(train_df, train_embed_file_name)\nprint(\"Finished at \", time.ctime())\n\ntest_df = pd.read_csv(test_df_path, sep='\\t').drop(columns=['URL',])\n# clean text\ntest_df = test_df.apply(entity_replace_func, axis=1)\n# clip text\ntext_clip_update(test_df, MAX_LEN)\nencode_by_bert(test_df, test_embed_file_name)\nprint(\"Finished at \", time.ctime())\n\n# dev_df = pd.read_csv(dev_df_path, sep='\\t').drop(columns=['URL',])\n# # clean text\n# dev_df = dev_df.apply(entity_replace_func, axis=1)\n# # clip text\n# text_clip_update(dev_df, MAX_LEN)\n# encode_by_bert(dev_df, dev_embed_file_name)\n# # split\n# split_embed_files(dev_embed_file_name, 32, embed_folder, \"dev_embed\")\n# os.system(\"rm \" + dev_embed_file_name)\n# print(\"Finished at \", time.ctime())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5eac3b67a93c6412e0a622300887dc645f4a88f2"},"cell_type":"code","source":"os.system(\"rm \" + \"uncased_L-12_H-768_A-12.zip\")\nos.system(\"rm -rdf \" + \"uncased_L-12_H-768_A-12\")\nos.system(\"rm \" + \"bert*\")\nos.system(\"rm \" + \"vocab.text\")\nos.system(\"rm \" + \"extract_features*\")\nos.system(\"rm \" + \"modeling*\")\nos.system(\"rm \" + \"tokenization*\")\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72d59c47c7a374a7d19c168ea380abb33d70325f"},"cell_type":"code","source":"# split\nsplit_embed_files(train_embed_file_name, batch_size, embed_folder, \"train_embed\")\nsplit_embed_files(test_embed_file_name, batch_size, embed_folder, \"test_embed\")\nprint(\"Finished at \", time.ctime())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a170f0c10c1a001c65fd546514f61c342448d64"},"cell_type":"code","source":"def _row_to_y(row):\n    if row.loc['A-coref']:\n        return 0\n    if row.loc['B-coref']:\n        return 1\n    return 2\n\ntrain_df['Label'] = train_df.apply(_row_to_y, axis=1)\n#dev_df['Label'] = dev_df.apply(_row_to_y, axis=1)\ntest_df['Label'] = test_df.apply(_row_to_y, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1260a9cb405e6a54322b0e880d0a3c38f375fb1c"},"cell_type":"markdown","source":"## Reduce Memory Sapce"},{"metadata":{"trusted":true,"_uuid":"aac9d220bd30c268e463dfbe871d0d1c16c48998"},"cell_type":"code","source":"focus_columns = ['Pronoun-index', 'A-index', 'B-index', 'Label']\ntrain_df = train_df[focus_columns]\n#dev_df = dev_df[focus_columns]\ntest_df = test_df[focus_columns]\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bb6a706145f87f5b40755aa209183f5f5e82a7b"},"cell_type":"markdown","source":"Now that we have the embeddings, we pass them to a multi-layer perceptron (i.e. vanilla neural network), which learns to classify the triples of embeddings (emb_A, emb_B,emb_P) as \"A\", \"B\" or \"NEITHER\"."},{"metadata":{"_uuid":"728fabda811e05ad9aa2b43602c147ac82c9ce42"},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true,"_uuid":"6d62c1f2a91014a0ad8d35628d92c81c228787bd"},"cell_type":"code","source":"import numpy as np\nfrom keras import backend\nfrom keras import layers\nfrom keras import models\n\nfrom keras import initializers, regularizers, constraints, activations\nfrom keras.engine import Layer\nimport keras.backend as K\nfrom keras.layers import merge\nfrom keras import callbacks as kc\nfrom keras import optimizers as ko\\\n\nfrom sklearn.model_selection import cross_val_score, KFold, train_test_split\nfrom sklearn.metrics import log_loss\n\nimport tensorflow as tf\n\nhistories = list()\nfile_paths = list()\ncos = list()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86458a3c857467b14bcc4420df80134bc41d6fca"},"cell_type":"markdown","source":"## Define Keras Layers"},{"metadata":{"trusted":true,"_uuid":"beda129560995c553efbad82e02550a7370ff335"},"cell_type":"code","source":"def _dot_product(x, kernel):\n    \"\"\"\n    Wrapper for dot product operation, in order to be compatible with both\n    Theano and Tensorflow\n    Args:\n        x (): input\n        kernel (): weights\n    Returns:\n    \"\"\"\n    if K.backend() == 'tensorflow':\n        # todo: check that this is correct\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)\n    \n    \nclass AttentionWeight(Layer):\n    \"\"\"\n        This code is a modified version of cbaziotis implementation:  GithubGist cbaziotis/AttentionWithContext.py\n        Attention operation, with a context/query vector, for temporal data.\n        Supports Masking.\n        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n        \"Hierarchical Attention Networks for Document Classification\"\n        by using a context vector to assist the attention\n        # Input shape\n            3D tensor with shape: `(samples, steps, features)`.\n        # Output shape\n            2D tensor with shape: `(samples, steps)`.\n        :param kwargs:\n        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n        The dimensions are inferred based on the output shape of the RNN.\n        Example:\n            model.add(LSTM(64, return_sequences=True))\n            model.add(AttentionWeight())\n        \"\"\"\n\n    def __init__(self,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWeight, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        shape1 = input_shape[0]\n        shape2 = input_shape[1]\n\n        self.W = self.add_weight((shape2[-1], shape1[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((shape2[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, inputs, mask=None):\n        x = inputs[0]\n        u = inputs[1]\n        \n        uit = _dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = K.batch_dot(uit, u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        \n        return a\n\n    def compute_output_shape(self, input_shape):\n        if not isinstance(input_shape, list) or len(input_shape) != 2:\n            raise ValueError('A `Dot` layer should be called '\n                             'on a list of 2 inputs.')\n        shape1 = list(input_shape[0])\n        shape2 = list(input_shape[1])\n        \n        return shape1[0], shape1[1]\n\n    def get_config(self):\n        config = {\n            'W_regularizer': regularizers.serialize(self.W_regularizer),\n            'b_regularizer': regularizers.serialize(self.b_regularizer),\n            'W_constraint': constraints.serialize(self.W_constraint),\n            'b_constraint': constraints.serialize(self.b_constraint),\n            'bias': self.bias\n        }\n        base_config = super(AttentionWeight, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n    \n    \nclass FeatureSelection1D(Layer):\n    \"\"\"\n        Normalize feature along a specific axis.\n        Supports Masking.\n\n        # Input shape\n            A ND tensor with shape: `(samples, timesteps, features)\n            A 2D tensor with shape: [samples, num_selected_features]\n        # Output shape\n            ND tensor with shape: `(samples, num_selected_features, features)`.\n        :param kwargs:\n        \"\"\"\n\n    def __init__(self, num_selects, **kwargs):\n\n        self.num_selects = num_selects\n        self.supports_masking = True\n        super(FeatureSelection1D, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n\n        super(FeatureSelection1D, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # don't pass the mask to the next layers\n        return None\n\n    def call(self, inputs, mask=None):\n        if not isinstance(inputs, list) or len(inputs) != 2:\n            raise ValueError('FeatureSelection1D layer should be called '\n                             'on a list of 2 inputs.')\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a = K.cast(mask, K.floatx()) * inputs[0]\n        else:\n            a = inputs[0]\n\n        b = inputs[1]\n\n        a = tf.batch_gather(\n            a, b\n        )\n\n        return a\n\n    def compute_output_shape(self, input_shape):\n        if not isinstance(input_shape, list) or len(input_shape) != 2:\n            raise ValueError('A `FeatureSelection1D` layer should be called '\n                             'on a list of 2 inputs.')\n        shape1 = list(input_shape[0])\n        shape2 = list(input_shape[1])\n\n        if shape2[0] != shape1[0]:\n            raise ValueError(\"batch size must be same\")\n\n        if shape2[1] != self.num_selects:\n            raise ValueError(\"must conform to the num_select\")\n\n        return (shape1[0], self.num_selects, shape1[2])\n\n    def get_config(self):\n        config = {\n            'num_selects': self.num_selects\n        }\n        base_config = super(FeatureSelection1D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"351a99cfb2b3616216fe17f78c4c70484704c2da"},"cell_type":"markdown","source":"## Define  Model"},{"metadata":{"trusted":true,"_uuid":"3830854f36930b1de19eb870e5109b08fb753a3e"},"cell_type":"code","source":"def build_mapped_mlp_model(\n        embed_dim, time_steps, extra_feature_dims, output_dim, model_dim, mlp_dim,\n        mlp_depth=1, embed_drop_out=0.5, drop_out=0.5,\n        gpu=False, return_customized_layers=False):\n    \n    # sequences inputs\n    input = models.Input(shape=(time_steps, embed_dim), dtype='float32', name='input')\n    x = input\n    \n    # mention position in the sentence\n    inputpi = models.Input(shape=(1,), dtype='int32', name='inputpi')\n    inputai = models.Input(shape=(1,), dtype='int32', name='inputai')\n    inputbi = models.Input(shape=(1,), dtype='int32', name='inputbi')\n    xis = [inputpi, inputai, inputbi]\n    \n    # addtional mention-pair features\n#     inputpa = models.Input(shape=(extra_feature_dims,), dtype='float32', name='inputpa')\n#     inputpb = models.Input(shape=(extra_feature_dims,), dtype='float32', name='inputpb')\n#     xextrs = [inputpa, inputpb]\n    \n    select_layer = FeatureSelection1D(1, name='boundary_selection_layer')\n    embed_bn = layers.BatchNormalization(name = 'embed_batch_norm_layer')\n    embed_dropout = layers.Dropout(embed_drop_out, name='embed_dropout_layer')\n    map_layer = layers.Dense(model_dim, activation=\"relu\", name=\"embed_map_layer\")\n    \n    flatten_layer1 = layers.Flatten('channels_first', name=\"flatten_layer1\")\n    \n    xs = list()\n    for i in range(len(xis)):\n        select_ = select_layer([x, xis[i]])\n        flatten_select_ = flatten_layer1(select_)\n        flatten_select_ = embed_bn(flatten_select_)\n        flatten_select_ = embed_dropout(flatten_select_)\n        flatten_select_ = map_layer(flatten_select_)\n        xs.append(flatten_select_)\n    \n#     feature_dropout_layer = layers.Dropout(rate=drop_out, name=\"feature_dropout_layer\")\n#     feature_map_layer = layers.Dense(model_dim, activation=\"relu\",name=\"feature_map_layer\")\n#     xextrs = [feature_map_layer(feature_dropout_layer(xextr)) for xextr in xextrs]\n    \n    #x = layers.Concatenate(axis=1, name=\"concat_feature_layer\")(xs + xextrs)\n    x = layers.Concatenate(axis=1, name=\"concat_feature_layer\")(xs)\n    x = layers.BatchNormalization(name = 'batch_norm_layer')(x)\n    x = layers.Dropout(drop_out, name='dropout_layer')(x)\n\n    # MLP Layers\n    for i in range(mlp_depth - 1):\n        x = layers.Dense(mlp_dim, activation='selu', kernel_initializer='lecun_normal', name='selu_layer' + str(i))(x)\n        x = layers.AlphaDropout(drop_out, name='alpha_layer' + str(i))(x)\n\n    outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"softmax_layer0\")(x)\n\n    model = models.Model([input,] + xis, outputs)\n\n    if return_customized_layers:\n        return model, {'FeatureSelection1D': FeatureSelection1D}\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd5954e526743f152af7c988480f3f069b3c3144"},"cell_type":"markdown","source":"## Prepare Data"},{"metadata":{"trusted":true,"_uuid":"689faa35707f836cc45e3d1c8d7a6d482337276a"},"cell_type":"code","source":"embed_dim = 768\ntime_steps = MAX_LEN + 10\nextra_feature_dims = 0\noutput_dim = 3\nmodel_dim = 20\nmlp_dim = 37\nmlp_depth=1\nembed_drop_out = 0.6\ndrop_out=0.3\ngpu = True\nreturn_customized_layers=True\nepochs = 40","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47885c9792f9b3cf441083ba2ba9c1ea6dcaa98e"},"cell_type":"code","source":"def pad_array(array_, max_len):\n    array_list = list()\n    for i in range(array_.shape[0]):\n        sub_array = array_[i]\n        if sub_array.shape[0] < max_len:\n            array_list.append(np.concatenate((sub_array, np.zeros(shape=(max_len - sub_array.shape[0], sub_array.shape[1])))))\n        elif sub_array.shape[0] == max_len:\n            array_list.append(sub_array)\n        else:\n            raise ValueError(\"max_len: \" + str(max_len) + \", actual: \" + str(sub_array.shape[0]))\n            \n    return np.asarray(array_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b076a7e861f3d9e432c0f53e1f784be628c8129f"},"cell_type":"code","source":"def data_generator(data_df, data_folder, dataset_name, batch_size, time_steps, batch_indices, return_label=True, shuffle=False):\n    \n    while True:\n        if shuffle:\n            np.random.shuffle(batch_indices)\n        \n        for batch_index in batch_indices:\n            data_indices = np.array(\n                list(range(batch_index * batch_size, min(batch_index * batch_size + batch_size, data_df.shape[0]))))\n\n            tmp_df = data_df.iloc[data_indices]\n            tmp_embed = np.load(batch_file_path(data_folder, dataset_name, batch_index, \".npy\"))\n            tmp_embed = pad_array(tmp_embed, time_steps)\n\n            if return_label:\n                yield [tmp_embed, tmp_df['Pronoun-index'].values, tmp_df['A-index'].values, tmp_df['B-index'].values], \\\n                      tmp_df['Label'].values\n            else:\n                yield [tmp_embed, tmp_df['Pronoun-index'].values, tmp_df['A-index'].values, tmp_df['B-index'].values]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4a4c62a346a98dadaa40766862ee2533b6d8ff9"},"cell_type":"code","source":"def load_data(data_df, data_folder, dataset_name, batch_size, time_steps, batch_indices):\n    \n    X_df = list()\n    embeds = list()\n    \n    for batch_index in batch_indices:\n        data_indices = np.array(list(range(batch_index*batch_size, min(batch_index*batch_size+batch_size, data_df.shape[0]))))\n        \n        tmp_df = data_df.iloc[data_indices]\n        tmp_embed = np.load(batch_file_path(data_folder, dataset_name, batch_index, \".npy\"))\n        tmp_embed = pad_array(tmp_embed, time_steps)\n        \n        X_df.append(tmp_df)\n        embeds.append(tmp_embed)\n        \n    X_df = pd.concat(X_df, axis=0, ignore_index=True)\n    X_df.reset_index(drop=True, inplace=True)\n    embeds = np.concatenate(tuple(embeds), axis=0)\n    \n    return [embeds, X_df['Pronoun-index'].values, X_df['A-index'].values, X_df['B-index'].values], X_df['Label'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4d2fe8bc0897c224de5343dfe7885749bb867f4"},"cell_type":"code","source":"def measure_log_loss(ground, preds):\n    preds = preds.tolist()\n    return skm.log_loss(ground, preds, labels=[0, 1, 2], eps=10**-15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86acc442b50d292a62d63625926670fa4bbabffe"},"cell_type":"markdown","source":"## Train Model"},{"metadata":{"trusted":true,"_uuid":"de22ced44b30836e5b92930e46295ba4a7c98ba4"},"cell_type":"code","source":"n_fold = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2538dcc94af4b17eb2c34112f62cf5244d2329ca"},"cell_type":"code","source":"# Training and cross-validation\nnum_batches = int(np.ceil(float(train_df.shape[0]) / batch_size))\ndummy_X = np.zeros(shape=(num_batches, 1))\n\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=3)\npreds = None\nfor fold_n, (train_batch_index, dev_batch_index) in enumerate(folds.split(dummy_X)):\n    # split training and validation data\n    print('Fold', fold_n, 'started at', time.ctime())\n    \n    # train and dev data\n    train_generator = data_generator(train_df, embed_folder, \"train_embed\", batch_size, time_steps, train_batch_index, return_label=True, shuffle=True)\n    X_dev, y_dev = load_data(train_df, embed_folder, \"train_embed\", batch_size, time_steps, dev_batch_index)\n    # test data\n    num_test_batches = int(np.ceil(float(test_df.shape[0]) / batch_size))\n    test_generator = data_generator(test_df, embed_folder, \"test_embed\", batch_size, time_steps, np.arange(num_test_batches), return_label=False)\n    \n    model, co = build_mapped_mlp_model(\n        embed_dim, time_steps, extra_feature_dims, output_dim, model_dim, mlp_dim,\n        mlp_depth=mlp_depth, embed_drop_out=embed_drop_out, drop_out=drop_out,\n        gpu=gpu, return_customized_layers=return_customized_layers)\n    \n    adam = ko.Nadam()\n    model.compile(adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n    \n    model_path = \"best_bert_atten_model.hdf5\"\n    check_point = kc.ModelCheckpoint(model_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n    early_stop = kc.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=3)\n    model.fit_generator(train_generator, steps_per_epoch=train_batch_index.shape[0], epochs=epochs, validation_data=(X_dev, y_dev), callbacks = [check_point, early_stop])\n    del model\n    gc.collect()\n    \n    print(\"load best model: \" + model_path)\n    model = models.load_model(model_path, co)\n    preds_tmp = model.predict_generator(test_generator, num_test_batches)\n    \n    print(\"single model accuracy: \")\n    print(measure_log_loss(test_df['Label'].values, preds_tmp))\n    \n    if preds is None:\n        preds = preds_tmp\n    else:\n        preds += preds_tmp\n    \npreds /= n_fold\nprint(\"cv model accuracy: \")\nprint(measure_log_loss(test_df['Label'].values, preds))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f67add7b13766d940873241d5ab7f0faf9853a7"},"cell_type":"markdown","source":"# Save Results"},{"metadata":{"trusted":true,"_uuid":"70573ac8acf0dd512b3fab1b862dad465e37c391"},"cell_type":"code","source":"sub_df_path = os.path.join(SUB_DATA_FOLDER, 'sample_submission_stage_1.csv')\nsub_df = pd.read_csv(sub_df_path)\nsub_df.loc[:, 'A'] = pd.Series(preds[:, 0])\nsub_df.loc[:, 'B'] = pd.Series(preds[:, 1])\nsub_df.loc[:, 'NEITHER'] = pd.Series(preds[:, 2])\n\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2f70d3c6cf615cd8e7b546273c369406129109a"},"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}